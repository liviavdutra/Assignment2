{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d187f185-77b1-41f1-a9ec-5f82496972d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pycocotools.coco import COCO\n",
    "except:\n",
    "    !pip install pycocotools\n",
    "    from pycocotools.coco import COCO\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de4b9b2-2faa-42e4-a40d-799f6ea451d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database.loading annotations into memory...\n",
      "\n",
      "Done (t=19.17s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(annotation_file=\"/scratch/lt2316-h18-resources/coco/annotations/instances_train2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0696f5-143a-4b9b-8572-4d0628d26792",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cat = coco.getCatIds(catNms=\"cat\")\n",
    "horse_cat = coco.getCatIds(catNms=\"horse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60686220-6ed7-4efb-9a3f-d8711834962c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17], [19])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cat, horse_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c95e6c-821b-4e33-9b06-8aa25991de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myOwnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, coco, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = coco\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([img_id])\n",
    "        # Size of bbox (Rectangular)\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        # Iscrowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271b8a6-3fd0-4441-82cf-b0d0c9c10774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927db768-2174-4b3d-ad8b-9d03305fff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your own data and coco file\n",
    "train_data_dir = '/scratch/lt2316-h18-resources/coco/train2017'\n",
    "val_data_dir = '/scratch/lt2316-h18-resources/coco/val2017'\n",
    "#train_coco = 'my_data/my_train_coco.json'\n",
    "\n",
    "# create own Dataset\n",
    "my_train_dataset = myOwnDataset(root=train_data_dir,\n",
    "                          coco = coco,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "my_val_dataset = myOwnDataset(root=val_data_dir,\n",
    "                          coco = coco, # será necessário criar uma coco da pasta de validation?\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Batch size\n",
    "train_batch_size = 1\n",
    "\n",
    "# own DataLoader\n",
    "dataloader_train = torch.utils.data.DataLoader(my_train_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(my_val_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f642f-9ae7-47a9-9701-b1e191007af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936a49e-4adc-4a8c-9cd5-78eabd11d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device (whether GPU or CPU)\n",
    "device = torch.device('cuda:2') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# DataLoader is iterable over Dataset\n",
    "#for imgs, annotations in data_loader:\n",
    "#    imgs = list(img.to(device) for img in imgs)\n",
    "#    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    #print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5ab46-499c-427a-aca9-12b4ed59f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from timm import create_model\n",
    "except:\n",
    "    !pip install timm\n",
    "    from timm import create_model\n",
    "\n",
    "# Definição do modelo ViT\n",
    "def create_vit_model():\n",
    "    model = create_model('vit_base_patch16_224', pretrained=True)  # ViT model\n",
    "    return model\n",
    "\n",
    "vit = create_vit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f3820-0069-43e7-9d85-f0a4a73a55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "coco_train = CocoDetection(root= train_data_dir, annFile='/scratch/lt2316-h18-resources/coco/annotations/instances_train2017.json', transform=None)\n",
    "coco_val = CocoDetection(root=val_data_dir, annFile='/scratch/lt2316-h18-resources/coco/annotations/instances_val2017.json', transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad1200-5c6a-4b6e-a911-bb76440f5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(coco_train, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(coco_val, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480b87c-78f2-414f-a6e2-b7b33571c3dc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4772929-bcad-48e7-a748-e4616d1a8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina a função de perda (loss function) e o otimizador\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vit.parameters(), lr=0.001)\n",
    "\n",
    "# Treinamento do modelo\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vit.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Avaliação do modelo\n",
    "    vit_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = vit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f1392-f080-4c7c-b52c-261e120ee6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39084afc-b584-4854-a313-eb334c834f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc8f5d-3025-47e9-83ac-9f7f609a5b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68faa2a8-1f3f-4f1b-8339-63a23805576b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc97ee8-5fd8-41a5-a821-a3917cfa25f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a4283-4404-4b8d-99b3-f0bbb0fa170a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2da57f-5ebd-4228-b19a-8262da089863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e0e4c-3129-42a4-a96e-a085d99b02e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277cd51-fab3-4a5b-9c4b-a643c2bbcfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed087b84-a19a-45d6-b18a-5937840dbf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118c45f-7b7f-4426-b8ef-9ca02e9a03c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4258d9c-0255-4e1f-9e50-a72ac51631e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c74d9e-a3a1-4565-89a0-b85a98ecc300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fd074-0d7e-4347-b0a1-35ac9c4e10a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b9745-5628-4c6b-abf6-4b0d51333f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb2dde-f2b1-43cf-87f4-2d632e795286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb31d70-2d79-45bc-a238-8375fabb39ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e70b11e-15f8-4566-8d28-e87bfa1fdc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9f0ff-a90d-4024-9f09-5cd2d04c995f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7b9db-da1b-4fc6-84cc-982d4ec3f7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398e5a1-ff6d-4806-8449-9c37173e8f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d8a2b-2ab4-4d4e-a544-35e25f9348cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3695af3-9631-47eb-b64c-3c4255a36bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ceb2d-a511-46b6-866c-3c12db86ac02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4effa6-e580-48bc-9ecb-f440756182b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfba71-3297-4890-a889-2d3431af5feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f585a-ed86-49bf-bd23-d942edfcd22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d84dba-778a-4d73-9e65-883365525f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da3575-52cc-40fe-a69b-61c08646e6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b98cdb-f2b7-458c-aa35-7edc31465348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
